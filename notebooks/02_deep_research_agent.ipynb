{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc7GKwVJ23R6"
      },
      "source": [
        "# Building Deep Research Agent \n",
        "\n",
        "This script will walk you through building on top of our simple tool calling agent to evolve it to a full Deep Research Agent. \n",
        "We will cover: \n",
        "1. Prompting strategies \n",
        "2. Multi-tool agents designg\n",
        "3. Compacting conversations \n",
        "\n",
        "Docs: \n",
        "Weights & Biases Inference [docs](https://docs.wandb.ai/guides/inference/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1huUI69O3bx"
      },
      "source": [
        "## Imports + API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9hK_PEje-0S-",
        "outputId": "62505f66-0072-40e2-e24e-601778a14d60"
      },
      "outputs": [],
      "source": [
        "#if you are running this on colab, uncomment the following line and run it\n",
        "#!uv pip install exa-py weave openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEiKMVKS8slD"
      },
      "outputs": [],
      "source": [
        "# Global Configuration & Setup\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import weave\n",
        "import openai\n",
        "from enum import Enum\n",
        "from pydantic import BaseModel, Field\n",
        "from rich.pretty import pprint\n",
        "from typing import Any, Callable, Dict, List, get_type_hints\n",
        "from exa_py import Exa\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hslss_PdY992"
      },
      "outputs": [],
      "source": [
        "#if you are running this on colab, uncomment the following lines and run it\n",
        "#from google.colab import userdata\n",
        "#EXA_API_KEY=userdata.get('EXA_API_KEY')\n",
        "#OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "#WANDB_API_KEY=userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# if you have a .secrets.sh file, uncomment the following lines and run it\n",
        "#EXA_API_KEY=os.getenv('EXA_API_KEY')\n",
        "#OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
        "#WANDB_API_KEY=os.getenv('WANDB_API_KEY')\n",
        "\n",
        "# if you use .env file, uncomment the following lines and run it\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "EXA_API_KEY=os.getenv('EXA_API_KEY')\n",
        "OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
        "WANDB_API_KEY=os.getenv('WANDB_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29IL8eNR73wn"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    # The custom base URL points to W&B Inference\n",
        "    #base_url='https://api.inference.wandb.ai/v1',\n",
        "\n",
        "    # Get your API key from https://wandb.ai/authorize\n",
        "    api_key=OPENAI_API_KEY,\n",
        "\n",
        "\n",
        "    # Optional: Team and project for usage tracking\n",
        "    #project=\"<your-team>/<your-project>\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "8e94c428730841c3bdaeaf82eb73381b",
            "2979e24129fb43f2b26af01503c7dadb"
          ]
        },
        "id": "vmGZXORMUy8f",
        "outputId": "aed86971-2ce8-4bce-a2d7-5efb8c807b11"
      },
      "outputs": [],
      "source": [
        "weave.init(\"wandb-applied-ai-team/london-workshop-2025\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcFtYhvCO9Bw"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2spvM-m81GJ"
      },
      "outputs": [],
      "source": [
        "def generate_tool_schema(func: Callable) -> dict:\n",
        "    \"\"\"Given a Python function, generate a tool-compatible JSON schema.\n",
        "    Handles basic types and Enums. Assumes docstrings are formatted for arg descriptions.\n",
        "    \"\"\"\n",
        "    signature = inspect.signature(func)\n",
        "    parameters = signature.parameters\n",
        "    type_hints = get_type_hints(func)\n",
        "\n",
        "    schema = {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": inspect.getdoc(func).split(\"\\\\n\")[0] if inspect.getdoc(func) else \"\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {},\n",
        "                \"required\": [],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    docstring = inspect.getdoc(func)\n",
        "    param_descriptions = {}\n",
        "    if docstring:\n",
        "        args_section = False\n",
        "        current_param = None\n",
        "        for line in docstring.split('\\\\n'):\n",
        "            line_stripped = line.strip()\n",
        "            if line_stripped.lower().startswith((\"args:\", \"arguments:\", \"parameters:\")):\n",
        "                args_section = True\n",
        "                continue\n",
        "            if args_section:\n",
        "                if \":\" in line_stripped:\n",
        "                    param_name, desc = line_stripped.split(\":\", 1)\n",
        "                    param_descriptions[param_name.strip()] = desc.strip()\n",
        "                elif line_stripped and not line_stripped.startswith(\" \"): # Heuristic: end of args section\n",
        "                     args_section = False\n",
        "\n",
        "    for name, param in parameters.items():\n",
        "        is_required = param.default == inspect.Parameter.empty\n",
        "        param_type = type_hints.get(name, Any)\n",
        "        json_type = \"string\"\n",
        "        param_schema = {}\n",
        "\n",
        "        # Basic type mapping\n",
        "        if param_type == str: json_type = \"string\"\n",
        "        elif param_type == int: json_type = \"integer\"\n",
        "        elif param_type == float: json_type = \"number\"\n",
        "        elif param_type == bool: json_type = \"boolean\"\n",
        "        elif hasattr(param_type, '__origin__') and param_type.__origin__ is list: # Handle List[type]\n",
        "             item_type = param_type.__args__[0] if param_type.__args__ else Any\n",
        "             if item_type == str: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "             elif item_type == int: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"integer\"}}\n",
        "             # Add more list item types if needed\n",
        "             else: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}} # Default list item type\n",
        "        elif hasattr(param_type, \"__members__\") and issubclass(param_type, Enum): # Handle Enum\n",
        "             json_type = \"string\"\n",
        "             param_schema[\"enum\"] = [e.value for e in param_type]\n",
        "\n",
        "        if not param_schema: # If not set by List or Enum\n",
        "            param_schema[\"type\"] = json_type\n",
        "\n",
        "        param_schema[\"description\"] = param_descriptions.get(name, \"\")\n",
        "\n",
        "        if param.default != inspect.Parameter.empty and param.default is not None:\n",
        "             param_schema[\"default\"] = param.default # Note: OpenAI schema doesn't officially use default, but useful metadata\n",
        "\n",
        "        schema[\"function\"][\"parameters\"][\"properties\"][name] = param_schema\n",
        "        if is_required:\n",
        "            schema[\"function\"][\"parameters\"][\"required\"].append(name)\n",
        "    return schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2777D3DLcEZ"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def call_model(model_name: str, messages: List[Dict[str, Any]], **kwargs) -> str:\n",
        "    \"Call a model with the given messages and kwargs.\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehkN-eK08kLS"
      },
      "outputs": [],
      "source": [
        "def function_tool(func: Callable) -> Callable:\n",
        "    \"\"\"Attaches a tool schema to the function and marks it as a tool.\n",
        "    Call this *after* defining your function: my_func = function_tool(my_func)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        func.tool_schema = generate_tool_schema(func)\n",
        "        func.is_tool = True # Mark it as a tool\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing tool {func.__name__}: {e}\")\n",
        "        # Optionally raise or mark as failed\n",
        "        func.tool_schema = None\n",
        "        func.is_tool = False\n",
        "    return func\n",
        "\n",
        "def get_tool(tools: list[Callable], name: str) -> Callable:\n",
        "    for t in tools:\n",
        "        if t.__name__ == name:\n",
        "            return t\n",
        "    raise KeyError(f\"No tool with name {name} found\")\n",
        "\n",
        "ToolCall = []\n",
        "\n",
        "@weave.op\n",
        "def perform_tool_calls(tools: list[Callable], tool_calls: list[ToolCall]) -> list[dict]:\n",
        "    \"Perform the tool calls and return the messages with the tool call results\"\n",
        "    messages = []\n",
        "    for tool_call in tool_calls:\n",
        "        print(f\"Performing tool call: {tool_call.function.name}\")\n",
        "        print(f\"  - Args: {tool_call.function.arguments}\")\n",
        "        function_name = tool_call.function.name\n",
        "        function_args = json.loads(tool_call.function.arguments)\n",
        "        tool = get_tool(tools, function_name)\n",
        "        tool_response = tool(**function_args)\n",
        "        print(f\"  - Response: {tool_response}\")\n",
        "        messages.append({\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": str(tool_response),\n",
        "        })\n",
        "    return messages\n",
        "\n",
        "class AgentState(BaseModel):\n",
        "    \"\"\"Manages the state of the agent.\"\"\"\n",
        "    messages: List[Dict[str, Any]] = Field(default_factory=list)\n",
        "    step: int = Field(default=0)\n",
        "    final_assistant_content: str | None = None # Populated at the end of a run\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Pc1RavUyx0"
      },
      "outputs": [],
      "source": [
        "def get_today_str() -> str:\n",
        "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
        "    return datetime.now().strftime(\"%a %b %-d, %Y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH2oK-UlTaOp"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr67sUm2TZmk"
      },
      "outputs": [],
      "source": [
        "DEEP_RESEARCH_AGENT_PROMPT = \"\"\"\n",
        "  You are a research assistant conducting research on the user's input topic. For context, today's date is {date}.                                                                                                        │\n",
        "\n",
        "  <Task>\n",
        "  Your job is to use tools to gather information about the user's input topic.\n",
        "  You can use any of the tools provided to you to find resources that can help answer the research question.\n",
        "  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
        "  Your response should be a thorough answer to the user's question, citing sources and reasoning, providing an overview of the facts or any gaps in the subject.\n",
        "  </Task>\n",
        "\n",
        "  <Available Tools>\n",
        "  You have access to two main tools:\n",
        "  1. **clarification**: For asking user clarifying questions if needed. If you have clarifying questions start with this.\n",
        "  2. **planning**: For planning the research.\n",
        "  2. **exa_search**: For conducting web searches to gather information\n",
        "  2. **think_tool**: For reflection and strategic planning during research\n",
        "\n",
        "  **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**\n",
        "  </Available Tools>\n",
        "\n",
        "  <Instructions>\n",
        "  Think like a human researcher with limited time. Follow these steps:\n",
        "\n",
        "  1. **Read the question carefully** - What specific information does the user need?\n",
        "  2. **Start with broader searches** - Use broad, comprehensive queries first\n",
        "  3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
        "  4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
        "  5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
        "  6. **Provide an answer** - At the end, always provide the answer from your research.\n",
        "  </Instructions>\n",
        "\n",
        "  <Hard Limits>\n",
        "  **Tool Call Budgets** (Prevent excessive searching):\n",
        "  - **Simple queries**: Use 2-3 search tool calls maximum\n",
        "  - **Complex queries**: Use up to 5 search tool calls maximum\n",
        "  - **Always stop**: After 5 search tool calls if you cannot find the right sources\n",
        "\n",
        "  **Stop Immediately When**:\n",
        "  - You can answer the user's question comprehensively\n",
        "  - You have 3+ relevant examples/sources for the question\n",
        "  - Your last 2 searches returned similar information\n",
        "  </Hard Limits>\n",
        "\n",
        "  <Show Your Thinking>\n",
        "  After each search tool call, use think_tool to analyze the results:\n",
        "  - What key information did I find?\n",
        "  - What's missing?\n",
        "  - Do I have enough to answer the question comprehensively?\n",
        "  - Should I search more or provide my answer?\n",
        "  </Show Your Thinking>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9fR1uLAPQER"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58zgUvgb9Qb0"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool\n",
        "def planning(plan: str) -> str:\n",
        "  \"\"\"Tool for planning the research.\n",
        "\n",
        "  Use this tool as the first step of the research.\n",
        "\n",
        "  Your plan should include:\n",
        "  1. Short analysis of user request.\n",
        "  2. Sub-queries broken down from users request, for example: if the query is 'what are 3 heaviest pokemons and their weight combined' the sub queries should be 'what are 3 heaviest pokemons' 'pokemon1 weight', 'pokemon2 weight', 'pokemon3 weight'.\n",
        "\n",
        "  Args:\n",
        "    plan: plan for the research.\n",
        "  \"\"\"\n",
        "\n",
        "  #return f\"The plan: {plan}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoa3s0hVXkGd"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool\n",
        "def clarification(clarifying_questions):\n",
        "  \"\"\"                                                                                                                                                                                                               │                                                                                                                 │\n",
        "  Use this tool to ask clarifying questions to the user.\n",
        "  IMPORTANT: If you can see in the messages history that you have already asked a clarifying question, you almost always do not need to ask another one. Only ask another question if ABSOLUTELY NECESSARY.\n",
        "\n",
        "  If there are acronyms, abbreviations, or unknown terms, ask the user to clarify.\n",
        "  If you need to ask a question, follow these guidelines:\n",
        "  - Be concise while gathering all necessary information.\n",
        "  - Only ask max 3 questions.\n",
        "  - Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner.\n",
        "  - Use bullet points or numbered lists if appropriate for clarity. Make sure that this uses markdown formatting and will be rendered correctly if the string output is passed to a markdown renderer.\n",
        "  - Don't ask for unnecessary information, or information that the user has already provided. If you can see that the user has already provided the information, do not ask for it again.\n",
        "\n",
        "  This tool will return the user clarifications.\n",
        "  \"\"\"\n",
        "  output = input(clarifying_questions)\n",
        "  return output\n",
        "# put time out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRwgw5OJ-SR_"
      },
      "outputs": [],
      "source": [
        "# for colab:\n",
        "#exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# for local:\n",
        "#exa = Exa(api_key=os.getenv(\"EXA_API_KEY\"))\n",
        "\n",
        "@weave.op\n",
        "@function_tool\n",
        "def exa_search(query: str) -> json:\n",
        "  \"\"\"Tool for web searching.\n",
        "\n",
        "  Use this tool to search the web for any information needed.\n",
        "\n",
        "  Args:\n",
        "    query: query to search the internet\n",
        "\n",
        "  Returns:\n",
        "    results: returns top 5 results with the summaries of the pages\n",
        "  \"\"\"\n",
        "  result = exa.search_and_contents(\n",
        "  query,\n",
        "  type = \"fast\",\n",
        "  num_results = 5,\n",
        "  summary = True,\n",
        ")\n",
        "  return result\n",
        "\n",
        "exa_search(\"what is the capital of france?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exa_client.search_and_contents(\"what is the capital of france?\", type=\"fast\", num_results=5, summary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tools import async_exa_search_and_refine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await async_exa_search_and_refine(query=\"what is the capital of france?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la2QI_1VPzCG"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool\n",
        "def think_tool(reflection: str) -> str:\n",
        "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
        "\n",
        "    Use this tool after each search to analyze results and plan next steps systematically.\n",
        "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
        "\n",
        "    When to use:\n",
        "    - After receiving search results: What key information did I find?\n",
        "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
        "    - When assessing research gaps: What specific information am I still missing?\n",
        "    - Before concluding research: Can I provide a complete answer now?\n",
        "\n",
        "    Reflection should address:\n",
        "    1. Analysis of current findings - What concrete information have I gathered?\n",
        "    2. Gap assessment - What crucial information is still missing?\n",
        "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
        "    4. Strategic decision - Should I continue searching or provide my answer?\n",
        "\n",
        "    Args:\n",
        "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
        "    \"\"\"\n",
        "    #return f\"Reflection recorded: {reflection}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk8FWy25KxqV"
      },
      "outputs": [],
      "source": [
        "ToolCall = [clarification, planning, exa_search, think_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si6BmYXWPs4d"
      },
      "source": [
        "##Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmTupblVPiWp"
      },
      "outputs": [],
      "source": [
        "class DeepResearchAgent:\n",
        "    \"\"\"A deep research agent class with tracing, state, and tool processing.\"\"\"\n",
        "    def __init__(self, model_name: str, system_message: str, tools: List[Callable]):\n",
        "        self.model_name = model_name\n",
        "        self.system_message = system_message\n",
        "        self.tools = [function_tool(t) for t in tools] # add schemas to the tools\n",
        "\n",
        "    @weave.op(name=\"DeepResearchAgent.step\") # Trace each step\n",
        "    def step(self, state: AgentState) -> AgentState:\n",
        "        step = state.step + 1\n",
        "        messages = state.messages\n",
        "        final_assistant_content = None\n",
        "        try:\n",
        "            # call model with tools\n",
        "            response = call_model(\n",
        "                model_name=self.model_name,\n",
        "                messages=messages,\n",
        "                tools=[t.tool_schema for t in self.tools])\n",
        "\n",
        "            # add the response to the messages\n",
        "            messages.append(response.model_dump())\n",
        "\n",
        "            # if the LLM requested tool calls, perform them\n",
        "            if response.tool_calls:\n",
        "                print(\"LLM requested tool calls:\")\n",
        "                # perform the tool calls\n",
        "                tool_outputs = perform_tool_calls(tools=[clarification, planning, think_tool, exa_search], tool_calls=response.tool_calls)\n",
        "                messages.extend(tool_outputs)\n",
        "\n",
        "            # LLM gave content response\n",
        "            else:\n",
        "                messages.append(response.model_dump())\n",
        "                final_assistant_content = response.content\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in Agent Step: {e}\")\n",
        "            # Add an error message to history to indicate failure\n",
        "            messages.append({\"role\": \"assistant\", \"content\": f\"Agent error in step: {str(e)}\"})\n",
        "            final_assistant_content = f\"Agent error in step {step}: {str(e)}\"\n",
        "        return AgentState(messages=messages, step=step, final_assistant_content=final_assistant_content)\n",
        "\n",
        "    @weave.op(name=\"DeepResearchAgent.run\")\n",
        "    def run(self, user_prompt: str, max_turns: int = 10) -> AgentState:\n",
        "        state = AgentState(messages=[\n",
        "            {\"role\": \"system\", \"content\": self.system_message},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}])\n",
        "        for _ in range(max_turns):\n",
        "            print(f\"--- Agent Loop Turn {state.step}/{max_turns} ---\")\n",
        "            state = self.step(state)\n",
        "            if state.final_assistant_content:\n",
        "                return state\n",
        "        return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd2u5T11PvOx"
      },
      "source": [
        "##Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8oCz5d7fCMy8",
        "outputId": "2516646f-ac15-449e-f61d-bd2254d297a3"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\tagent = DeepResearchAgent(\n",
        "\t\tmodel_name=\"gpt-5-mini\",\n",
        "\t\tsystem_message=DEEP_RESEARCH_AGENT_PROMPT.format(date=get_today_str()),\n",
        "\t\ttools=[clarification, planning, think_tool, exa_search]\n",
        "\t)\n",
        "\tstate = agent.run(user_prompt=\"What type of vegan milk alternative is the healthiest?\")\n",
        "\tprint(f\"Final response: {state.final_assistant_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_5IL_dXexSz"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2979e24129fb43f2b26af01503c7dadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e94c428730841c3bdaeaf82eb73381b": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2979e24129fb43f2b26af01503c7dadb",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
