{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85671824",
   "metadata": {},
   "source": [
    "# Introduction to Building LLM Agents with Tools and Tracing\n",
    "\n",
    "<!--- @wandbcode{fc-london-workshop-2025} -->\n",
    "\n",
    "This script walks through the process of building a simple LLM-powered agent that can use tools (functions) to answer questions. We'll cover:\n",
    "1. Making basic LLM calls.\n",
    "2. Introducing Weave for tracing and observability.\n",
    "3. Defining tools for the LLM (manually and automatically).\n",
    "4. Implementing a basic agentic loop.\n",
    "5. Structuring the agent using Python classes.\n",
    "6. Running the agent on a multi-step task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fbc4f",
   "metadata": {},
   "source": [
    "**Prerequisites:**\n",
    "Make sure you have the necessary libraries installed:\n",
    "```bash\n",
    "!pip install weave openai\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d103af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration & Setup\n",
    "import os\n",
    "import inspect\n",
    "import json\n",
    "import weave # Must import weave before litellm for auto-patching\n",
    "import openai\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "from rich.console import Console as RichConsole\n",
    "from exa_py import Exa\n",
    "from typing import Any, Callable, Dict, List, get_type_hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf6667",
   "metadata": {},
   "source": [
    "Define a model to use, as we are going to use tool calling you need a capable model like `Kimi-K2` or `GLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Console(RichConsole):\n",
    "    def md(self, text): \n",
    "        return self.print(Markdown(text))\n",
    "\n",
    "console = Console()\n",
    "\n",
    "MODEL_SMALL = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n",
    "MODEL_MEDIUM = \"zai-org/GLM-4.5\"\n",
    "MODEL_LARGE = \"moonshotai/Kimi-K2-Instruct\"\n",
    "\n",
    "WANDB_ENTITY = \"wandb-applied-ai-team\"\n",
    "WANDB_PROJECT = \"london-workshop-2025\"\n",
    "\n",
    "oai_client = openai.OpenAI(\n",
    "    base_url='https://api.inference.wandb.ai/v1',\n",
    "    api_key=os.getenv(\"WANDB_API_KEY\"),\n",
    "    project=f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "exa_client = Exa(api_key=os.getenv(\"EXA_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c55be",
   "metadata": {},
   "source": [
    "Let's log to [W&B Weave](https://weave-docs.wandb.ai/). Weights & Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow:\n",
    "\n",
    "- Tracing & Monitoring: Track LLM calls and application logic to debug and analyze production systems.\n",
    "- Systematic Iteration: Refine and iterate on prompts, datasets, and models.\n",
    "- Experimentation: Experiment with different models and prompts in the LLM Playground.\n",
    "- Evaluation: Use custom or pre-built scorers alongside our comparison tools to systematically assess and enhance application performance.\n",
    "- Guardrails: Protect your application with pre- and post-safeguards for content moderation, prompt safety, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b57d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a Weave project. Traces will be sent here.\n",
    "# You can view them in the Weave UI (usually runs locally).\n",
    "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13266e05",
   "metadata": {},
   "source": [
    "## 1. Basic LLM Call with OpenAI SDK\n",
    "\n",
    "Let's start with a simple call to the LLM using/\n",
    "\n",
    "![](images/01_trace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple message list (conversation history)\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, LLM! How does an AI agent work?\"}]\n",
    "\n",
    "# Make the call\n",
    "response = resp = oai_client.chat.completions.create(\n",
    "    model = MODEL_SMALL,\n",
    "    messages=messages,\n",
    ")\n",
    "# Print the response content\n",
    "assistant_response = response.choices[0].message.content\n",
    "console.md(f\"LLM Response:\\\\n{assistant_response}\")\n",
    "\n",
    "# Click on the 🍩 linke below to see the trace in Weave 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ead84",
   "metadata": {},
   "source": [
    "Because we imported `weave` and called `weave.init()`, the OpenAI SDK call above was automatically traced. You can open your Weave dashboard and see the trace, including the input messages, output response, latency, model used, etc. This is invaluable for debugging and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the time you would want to define your own operations to trace, for instance to call the model.\n",
    "# You just need to add the @weave.op decorator to the function and it will be traced.\n",
    "\n",
    "@weave.op\n",
    "def call_model(model_name: str, messages: List[Dict[str, Any]], **kwargs) -> str:\n",
    "    \"Call a model with the given messages and kwargs.\"\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message\n",
    "\n",
    "response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
    "console.md(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97c8b6",
   "metadata": {},
   "source": [
    "![](images/02_nested_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75409570",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Introducing Tool Calling\n",
    "\n",
    "Agents become much more powerful when they can use **tools** – external functions or APIs – to get information or perform actions beyond the LLM's internal knowledge. To allow an LLM to use a tool, we need to provide it with a description (schema) of the tool, including its name, purpose, and expected arguments.\n",
    "\n",
    "Check the Mistral docs for function calling: https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "![](images/function-calling-diagram-steps.png)\n",
    "\n",
    "First, let's define a simple Python function we want the LLM to be able to call. We add `@weave.op` to trace when this function actually gets executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op \n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Use this tool to add numbers.\n",
    "    Args:\n",
    "        a: The first number.\n",
    "        b: The second number.\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numbers(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't work...\n",
    "call_model(model_name=MODEL_SMALL, messages=messages, tools=[add_numbers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082a58a",
   "metadata": {},
   "source": [
    "> We need to manually create the JSON schema describing this tool in a format that models *Mistral* understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define the tool schema\n",
    "tool_add_numbers_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"add_numbers\",\n",
    "        \"description\": \"Adds two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6dfd4",
   "metadata": {},
   "source": [
    "Now, we make an LLM call, passing the `tools` parameter with our schema. We ask a question that should trigger the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant use tools to answer questions.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My lucky numbers are 77 and 11. What is their sum?\"}]\n",
    "response = call_model(model_name=MODEL_SMALL, messages=messages, tools=[tool_add_numbers_schema])\n",
    "console.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fdded",
   "metadata": {},
   "source": [
    "## Manual Tool Call\n",
    "The LLM's response might contain a request to call our tool (`response.choices[0].message.tool_calls`) or it might respond directly (`response.choices[0].message.content`). If it requests a tool call, we need to:\n",
    "\n",
    "1. Parse the arguments it provides.\n",
    "2. Execute our actual Python function (`add_numbers`) with those arguments.\n",
    "3. (In a real agent loop) Send the result back to the LLM in a new message with `role=\"tool\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1ea6c",
   "metadata": {},
   "source": [
    "Let's manually call the tools in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e64e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.tool_calls:\n",
    "    console.print(\"LLM requested a tool call:\")\n",
    "    for tool_call in response.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args_str = tool_call.function.arguments\n",
    "        function_args = json.loads(function_args_str)\n",
    "        console.print(f\"  - Tool: {function_name}, Args: {function_args_str}\")\n",
    "        if function_name == \"add_numbers\":\n",
    "            tool_result_content = add_numbers(**function_args)\n",
    "\n",
    "console.print(f\"Final Result: {tool_result_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f38a9f",
   "metadata": {},
   "source": [
    "We need to add the tool call result to the messages (there is actually 2 messages to add)\n",
    "- the response from the assistant that decided to call the tool\n",
    "- the tool output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ca3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"role\": \"tool\",\n",
    "    \"name\": function_name,\n",
    "    \"content\": str(tool_result_content)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0992e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c76bf",
   "metadata": {},
   "source": [
    "You should have a sequence of messages like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5621c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[m[\"role\"] for m in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4b05f",
   "metadata": {},
   "source": [
    "Now call the model again with the new messages and it will use the tool call result to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf845aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
    "console.print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01abc5",
   "metadata": {},
   "source": [
    "## 3. Simplifying Tool Definition with a Processor Function\n",
    "\n",
    "Manually writing JSON schemas is tedious and error-prone. We can automate this by inspecting our Python function's signature, type hints, and docstring.\n",
    "\n",
    "First, let's define a helper function (`generate_tool_schema`) that takes a Python function and generates the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tool_schema(func: Callable) -> dict:\n",
    "    \"\"\"Given a Python function, generate a tool-compatible JSON schema.\n",
    "    Handles basic types and Enums. Assumes docstrings are formatted for arg descriptions.\n",
    "    \"\"\"\n",
    "    signature = inspect.signature(func)\n",
    "    parameters = signature.parameters\n",
    "    type_hints = get_type_hints(func)\n",
    "\n",
    "    schema = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": inspect.getdoc(func).split(\"\\\\n\")[0] if inspect.getdoc(func) else \"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": [],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    docstring = inspect.getdoc(func)\n",
    "    param_descriptions = {}\n",
    "    if docstring:\n",
    "        args_section = False\n",
    "        current_param = None\n",
    "        for line in docstring.split('\\\\n'):\n",
    "            line_stripped = line.strip()\n",
    "            if line_stripped.lower().startswith((\"args:\", \"arguments:\", \"parameters:\")):\n",
    "                args_section = True\n",
    "                continue\n",
    "            if args_section:\n",
    "                if \":\" in line_stripped:\n",
    "                    param_name, desc = line_stripped.split(\":\", 1)\n",
    "                    param_descriptions[param_name.strip()] = desc.strip()\n",
    "                elif line_stripped and not line_stripped.startswith(\" \"): # Heuristic: end of args section\n",
    "                     args_section = False\n",
    "\n",
    "    for name, param in parameters.items():\n",
    "        is_required = param.default == inspect.Parameter.empty\n",
    "        param_type = type_hints.get(name, Any)\n",
    "        json_type = \"string\"\n",
    "        param_schema = {}\n",
    "\n",
    "        # Basic type mapping\n",
    "        if param_type == str: json_type = \"string\"\n",
    "        elif param_type == int: json_type = \"integer\"\n",
    "        elif param_type == float: json_type = \"number\"\n",
    "        elif param_type == bool: json_type = \"boolean\"\n",
    "        elif hasattr(param_type, '__origin__') and param_type.__origin__ is list: # Handle List[type]\n",
    "             item_type = param_type.__args__[0] if param_type.__args__ else Any\n",
    "             if item_type == str: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "             elif item_type == int: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"integer\"}}\n",
    "             # Add more list item types if needed\n",
    "             else: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}} # Default list item type\n",
    "        elif hasattr(param_type, \"__members__\") and issubclass(param_type, Enum): # Handle Enum\n",
    "             json_type = \"string\"\n",
    "             param_schema[\"enum\"] = [e.value for e in param_type]\n",
    "\n",
    "        if not param_schema: # If not set by List or Enum\n",
    "            param_schema[\"type\"] = json_type\n",
    "\n",
    "        param_schema[\"description\"] = param_descriptions.get(name, \"\")\n",
    "\n",
    "        if param.default != inspect.Parameter.empty and param.default is not None:\n",
    "             param_schema[\"default\"] = param.default # Note: OpenAI schema doesn't officially use default, but useful metadata\n",
    "\n",
    "        schema[\"function\"][\"parameters\"][\"properties\"][name] = param_schema\n",
    "        if is_required:\n",
    "            schema[\"function\"][\"parameters\"][\"required\"].append(name)\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d193839",
   "metadata": {},
   "source": [
    "Now we can use this function to automatically generate the schema for our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_schema = generate_tool_schema(add_numbers)\n",
    "console.print(tool_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867416e",
   "metadata": {},
   "source": [
    "Now, we define a `function_tool` \"processor\". This isn't a decorator in the `@` syntax sense here, but a function that we call *after* defining our tool function. It uses `generate_tool_schema` to attach the schema to the function object itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ed9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tool(func: Callable) -> Callable:\n",
    "    \"\"\"Attaches a tool schema to the function and marks it as a tool.\n",
    "    Call this *after* defining your function: my_func = function_tool(my_func)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        func.tool_schema = generate_tool_schema(func)\n",
    "        func.is_tool = True # Mark it as a tool\n",
    "    except Exception as e:\n",
    "        console.print(f\"Error processing tool {func.__name__}: {e}\")\n",
    "        # Optionally raise or mark as failed\n",
    "        func.tool_schema = None\n",
    "        func.is_tool = False\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504243c6",
   "metadata": {},
   "source": [
    "We can use this function to automatically generate the schema for our tool, as a decorator or after the function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c424f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numbers = function_tool(add_numbers)\n",
    "console.print(add_numbers.tool_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c651811",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numbers.tool_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7280921",
   "metadata": {},
   "source": [
    "and call the tool =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numbers(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ef16e",
   "metadata": {},
   "source": [
    "### 3.1 Real Example using an API based tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227a8a9",
   "metadata": {},
   "source": [
    "We are going to use the [EXA search API](https://docs.exa.ai/reference/getting-started).\n",
    "- How does [EXA search works](https://docs.exa.ai/reference/how-exa-search-works#how-exa-search-works)\n",
    "- Using exa search [as tool calling](https://docs.exa.ai/reference/openai-tool-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0335e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Recipes for cooking seabass?\"\n",
    "\n",
    "search_res = exa_client.search_and_contents(query=query, type='auto', num_results=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(search_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e67727",
   "metadata": {},
   "source": [
    "Let's explore the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea673071",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.md(\"\\n-------------------\\n\".join(result.text for result in search_res.results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op \n",
    "@function_tool # <- we can use the decorator to automatically generate the tool schema\n",
    "def exa_search(query: str, num_results: int = 5) -> list[dict[str, str]]:\n",
    "    \"\"\"Perform a search query on the web and retrieve the most relevant URLs and web content.\n",
    "    \n",
    "    This function uses the Exa search API to find relevant web pages based on the query\n",
    "    and returns their titles, text content, and URLs.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query. Use detailed, specific queries for better results.\n",
    "               The quality of results depends on the specificity of the query.\n",
    "        num_results: The number of search results to retrieve. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "            - title: The title of the web page\n",
    "            - text: The text content of the web page\n",
    "            - url: The URL of the web page\n",
    "    \"\"\"\n",
    "    search_results = exa_client.search_and_contents(query=query, type='auto', num_results=num_results)\n",
    "    \n",
    "    output = []\n",
    "    for result in search_results.results:\n",
    "        output.append(\n",
    "            {\"title\": result.title,\n",
    "            \"text\": result.text,\n",
    "            \"url\": result.url\n",
    "            }\n",
    "        )\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f007ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "exa_search.tool_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3973765",
   "metadata": {},
   "source": [
    "We get a list of results with the relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = exa_search(\"How do I cook seabass?\")\n",
    "console.print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a073e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Return in markdown format.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I cook seabass?\"}]\n",
    "\n",
    "response = call_model(model_name=MODEL_SMALL, messages=messages, tools=[exa_search.tool_schema])\n",
    "console.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7350625",
   "metadata": {},
   "source": [
    "Let's create some helper functions to perform the tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion_message_function_tool_call import ChatCompletionMessageFunctionToolCall\n",
    "\n",
    "def get_tool(tools: list[Callable], name: str) -> Callable:\n",
    "    for t in tools:\n",
    "        if t.__name__ == name:\n",
    "            return t\n",
    "    raise KeyError(f\"No tool with name {name} found\")\n",
    "\n",
    "def perform_tool_calls(tools: list[Callable], tool_calls: list[ChatCompletionMessageFunctionToolCall]) -> list[dict]:\n",
    "    \"Perform the tool calls and return the messages with the tool call results\"\n",
    "    messages = []\n",
    "    if not tool_calls:\n",
    "        return messages\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        with console.status(f\"[bold cyan]Executing {function_name}...[/bold cyan]\"):\n",
    "            tool = get_tool(tools, function_name)\n",
    "            tool_response = tool(**function_args) # doesn't handle async\n",
    "        \n",
    "        # Create panel content\n",
    "        panel_content = f\"[bold cyan]🔧 Tool Call:[/bold cyan] {function_name}\\n\\n\"\n",
    "        panel_content += f\"[dim]Args: {tool_call.function.arguments}[/dim]\\n\\n\"\n",
    "        \n",
    "        if isinstance(tool_response, list):\n",
    "            panel_content += f\"[green]✓[/green] Found {len(tool_response)} results\"\n",
    "        else:\n",
    "            panel_content += f\"[green]✓[/green] {function_name} executed successfully\"\n",
    "        \n",
    "        console.print(Panel(panel_content, border_style=\"cyan\"))\n",
    "        \n",
    "        messages.append({\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": str(tool_response),\n",
    "        })\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5881ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add the tool call result to the messages\n",
    "messages.append(response.model_dump())\n",
    "messages.extend(perform_tool_calls(tools=[exa_search], tool_calls=response.tool_calls))\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Answer my previous query based on the search results.\",})\n",
    "\n",
    "final_response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
    "console.rule(\"Final Model Response\")\n",
    "console.md(final_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3956939",
   "metadata": {},
   "source": [
    "Let's wrap this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def research(query: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Return in markdown format.\"},\n",
    "        {\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    # call model with tools\n",
    "    response = call_model(\n",
    "        model_name=MODEL_SMALL, \n",
    "        messages=messages, \n",
    "        tools=[exa_search.tool_schema])\n",
    "\n",
    "    # add the response to the messages\n",
    "    messages.append(response.model_dump())\n",
    "\n",
    "    # perform the tool calls\n",
    "    messages.extend(perform_tool_calls(tools=[exa_search], tool_calls=response.tool_calls))\n",
    "    \n",
    "    # prompt the model to be grounded\n",
    "    messages.append({\"role\": \"user\",\"content\": \"Answer my previous query based on the search results.\",})\n",
    "\n",
    "    final_response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
    "    return final_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0183b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = research(\"What are the most popular pokemons?\")\n",
    "console.md(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa4a4",
   "metadata": {},
   "source": [
    "![](images/04_pokedex.png)\n",
    "\n",
    "This is \"Almost\" an agent, but it's missing the loop. Let's add that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd890f5",
   "metadata": {},
   "source": [
    "## 4. Implementing a Basic Agentic Loop\n",
    "\n",
    "Let's implement a basic agentic loop. We'll use the `pokedex` function we just created. The implementation we have above has some limitations:\n",
    "- Its a single turn, so if it fails to answer my question in one pass it is over.\n",
    "\n",
    "![](images/05_agent.png)\n",
    "\n",
    "From the really good [Anthropic Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) article and encourage people to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737003e",
   "metadata": {},
   "source": [
    "A simple for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def research_loop(query: str, max_turns: int = 4, tools = [exa_search, add_numbers]) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\"},\n",
    "        {\"role\": \"user\", \"content\": query}]\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        console.rule(f\"Agent Loop Turn {turn + 1}/{max_turns}\")\n",
    "\n",
    "        # call model with tools\n",
    "        response = call_model(\n",
    "            model_name=MODEL_MEDIUM, \n",
    "            messages=messages, \n",
    "            tools=[t.tool_schema for t in tools])\n",
    "\n",
    "        # add the response to the messages\n",
    "        messages.append(response.model_dump())\n",
    "\n",
    "        # if the LLM requested tool calls, perform them\n",
    "        if response.tool_calls:\n",
    "            # perform the tool calls\n",
    "            tool_outputs = perform_tool_calls(tools=[exa_search, add_numbers], tool_calls=response.tool_calls)\n",
    "            messages.extend(tool_outputs)\n",
    "        # LLM gave content response\n",
    "        elif response.content:\n",
    "            console.rule(\"Final Model Response\")\n",
    "            console.md(response.content)\n",
    "            return response.content\n",
    "        else:\n",
    "            print(\"LLM response had neither content nor tool calls. Stopping loop.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecad3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = research_loop(\"What is the sum of the populations of the 2 major EU cities?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e589c3",
   "metadata": {},
   "source": [
    "# 5. Structuring the Agent with Classes\n",
    "\n",
    "The loop above works, but for more complex agents, encapsulating the logic and state within classes is much better. We'll define:\n",
    "- `AgentState`: A Pydantic model to hold the conversation history and potentially other state.\n",
    "- `SimpleAgent`: A class containing the agent's configuration (model, system message, tools) and logic (`step`, `run`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f65412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(BaseModel):\n",
    "    \"\"\"Manages the state of the agent.\"\"\"\n",
    "    messages: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    step: int = Field(default=0)\n",
    "    final_assistant_content: str | None = None # Populated at the end of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f57c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    \"\"\"A simple agent class with tracing, state, and tool processing.\"\"\"\n",
    "    def __init__(self, model_name: str, system_message: str, tools: List[Callable]):\n",
    "        self.model_name = model_name\n",
    "        self.system_message = system_message\n",
    "        self.tools = [function_tool(t) for t in tools] # add schemas to the tools\n",
    "    \n",
    "    @weave.op(name=\"SimpleAgent.step\") # Trace each step\n",
    "    def step(self, state: AgentState) -> AgentState:\n",
    "        step = state.step + 1\n",
    "        messages = state.messages\n",
    "        final_assistant_content = None\n",
    "        try:\n",
    "            # call model with tools\n",
    "            response = call_model(\n",
    "                model_name=self.model_name, \n",
    "                messages=messages, \n",
    "                tools=[t.tool_schema for t in self.tools])\n",
    "\n",
    "            # add the response to the messages\n",
    "            messages.append(response.model_dump())\n",
    "\n",
    "            # if the LLM requested tool calls, perform them\n",
    "            if response.tool_calls:\n",
    "                # perform the tool calls\n",
    "                tool_outputs = perform_tool_calls(tools=self.tools, tool_calls=response.tool_calls)\n",
    "                messages.extend(tool_outputs)\n",
    "\n",
    "            # LLM gave content response\n",
    "            else:\n",
    "                messages.append(response.model_dump())\n",
    "                final_assistant_content = response.content\n",
    "        except Exception as e:\n",
    "            console.print(f\"ERROR in Agent Step: {e}\")\n",
    "            # Add an error message to history to indicate failure\n",
    "            messages.append({\"role\": \"assistant\", \"content\": f\"Agent error in step: {str(e)}\"})\n",
    "            final_assistant_content = f\"Agent error in step {step}: {str(e)}\"\n",
    "        return AgentState(messages=messages, step=step, final_assistant_content=final_assistant_content)\n",
    "\n",
    "    @weave.op(name=\"SimpleAgent.run\")\n",
    "    def run(self, user_prompt: str, max_turns: int = 10) -> AgentState:\n",
    "        state = AgentState(messages=[\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}])\n",
    "        for _ in range(max_turns):\n",
    "            console.rule(f\"Agent Loop Turn {state.step+1}/{max_turns}\")\n",
    "            state = self.step(state)\n",
    "            if state.final_assistant_content:\n",
    "                return state\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80842566",
   "metadata": {},
   "source": [
    "![](images/07_simple_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce6b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(\n",
    "    model_name=MODEL_SMALL,\n",
    "    system_message=\"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\",\n",
    "    tools=[exa_search, add_numbers]\n",
    ")\n",
    "state = agent.run(user_prompt=\"What is the combined weight of Ash's first 3 pokemons?\")\n",
    "print(f\"Final response: {state.final_assistant_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op \n",
    "@function_tool # <- we can use the decorator to automatically generate the tool schema\n",
    "def exa_search_and_refine(query: str, num_results: int = 5) -> list[dict[str, str]]:\n",
    "    \"\"\"Perform a search query on the web and retrieve the most relevant URLs and web content.\n",
    "    \n",
    "    This function uses the Exa search API to find relevant web pages based on the query\n",
    "    and returns their titles, text content, and URLs.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query. Use detailed, specific queries for better results.\n",
    "               The quality of results depends on the specificity of the query.\n",
    "        num_results: The number of search results to retrieve. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "            - title: The title of the web page\n",
    "            - text: The text content of the web page\n",
    "            - url: The URL of the web page\n",
    "    \"\"\"\n",
    "    search_results = exa_client.search_and_contents(query=query, type='auto', num_results=num_results)\n",
    "    \n",
    "    @weave.op\n",
    "    def refine_search_result(result, query):\n",
    "        messages = [\n",
    "            {\"role\":\"system\", \"content\": f\"Your task is to extract from the search results only the info that is relevant to answer the query\"},\n",
    "            {\"role\": \"user\", \"content\": f\"- query: {query}\\n- Search result: {result}\"}\n",
    "        ]\n",
    "        refined_search = call_model(model_name=MODEL_SMALL, messages=messages)\n",
    "        return refined_search.content\n",
    "\n",
    "    output = []\n",
    "    for item, result in enumerate(search_results.results):\n",
    "        console.print(f\"Refining result {item+1}\")\n",
    "        refined_text = refine_search_result(result.text, query)\n",
    "        output.append(\n",
    "            {\"title\": result.title,\n",
    "            \"text\": refined_text,\n",
    "            \"url\": result.url\n",
    "            }\n",
    "        )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(\n",
    "    model_name=MODEL_SMALL,\n",
    "    system_message=\"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\",\n",
    "    tools=[exa_search_and_refine, add_numbers]\n",
    ")\n",
    "state = agent.run(user_prompt=\"What is the combined weight of Ash's first 3 pokemons?\")\n",
    "print(f\"Final response: {state.final_assistant_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cd3c5",
   "metadata": {},
   "source": [
    "Possible improvements to the SimpleAgent:\n",
    "- Give the model info about the state of the conversation, you could inject a message with the model context pressure, steps left, etc.\n",
    "- Structured output. Make the model output a specific format, for instance a JSON with the expected fields.\n",
    "- Add more tools like read and write files, access a database.\n",
    "- Agent handoff: Agent1 does triage and Agent2 executes specific tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
