{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/morganmcg1/deep-research-bot/blob/main/notebooks/03_rl_for_deep_research_agent.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCF1aKktr7wM"
      },
      "source": [
        "# 3. Agent Reinforcement Learning - with Serverless RL\n",
        "\n",
        "This notebook shows how to RL fine-tune a Qwen-314B model to work well in a research agent using [OpenPipe's ART framework](https://github.com/OpenPipe/ART/) and W&B's Serverless RL. It will demonstrate how to set up a multi-turn agent and how to train it.\n",
        "\n",
        "To train this agent, click **Runtime** > **Run all**. Make sure you've set your `WANDB_API_KEY` and `EXA_API_KEY` below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqcaSOtFr7wN"
      },
      "source": [
        "## Getting Started\n",
        "### Environment Variables\n",
        "\n",
        "Later on in the notebook, we'll be creating a model that can automatically logs metrics to Weights & Biases and chat completions to Weave. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n",
        "\n",
        "*If you don't already have a W&B API key, you can get one [here](https://wandb.ai/authorize).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE! We'll be install from a branch here, not main\n",
        "!uv pip install -qqq git+https://github.com/morganmcg1/deep-research-bot.git@add_rl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add your WANDB_API_KEY, EXA_API_KEY and WANDB_ENTITY key here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "EXA_API_KEY=\"\"\n",
        "WANDB_API_KEY=\"\"\n",
        "\n",
        "\n",
        "WANDB_ENTITY = \"\"\n",
        "WANDB_PROJECT = \"london-workshop-2025-rl\"\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyZkABN7r7wO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import weave\n",
        "\n",
        "if not os.environ.get(\"WANDB_API_KEY\") or not os.environ.get(\"EXA_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"WANDB_API_KEY and EXA_API_KEY are required for inference, training, and logging to Weights & Biases.\"\n",
        "    )\n",
        "\n",
        "# Login to W&B Weave so that our rollouts are traced in the W&B Weave UI\n",
        "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\", settings={\"print_call_link\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-0v2nH7fPXk"
      },
      "source": [
        "## Set up the data\n",
        "\n",
        "We'll pass a list of tasks, these are the questions we want to train our agent model on. Our rollout will then take the question from each task and pass it to the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LXKuqMa-Udm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import art\n",
        "import weave\n",
        "from art.serverless.backend import ServerlessBackend\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "class Task(BaseModel):\n",
        "    question: str = Field(...)\n",
        "\n",
        "\n",
        "# Quick questions for testing\n",
        "# questions = [\n",
        "#     \"How can we have more cats?\",\n",
        "#     \"How can we have more dogs?\",\n",
        "#     \"How can we have more fish?\",\n",
        "#     \"How can we have more rabbits?\",\n",
        "#     \"How can we have more birds?\",\n",
        "#     \"How can we have more frogs?\",\n",
        "#     \"How can we have more lizards?\",\n",
        "#     \"How can we have more penguins?\",\n",
        "# ]\n",
        "\n",
        "# Deep Research-like questions\n",
        "questions = [\n",
        "    \"Investigate the emerging field of neuromorphic computing and its potential to revolutionize energy efficiency in artificial intelligence systems. Compare current implementations like Intel's Loihi and IBM's TrueNorth chips, analyze their architectural differences from traditional von Neumann systems, and evaluate the challenges in programming these brain-inspired processors. What breakthroughs are needed to make neuromorphic computing commercially viable for edge AI applications?\",\n",
        "    \"Analyze the psychological and sociological implications of virtual influencers and AI-generated personalities dominating social media platforms. How do parasocial relationships with non-human entities differ from those with real influencers? What are the ethical considerations regarding disclosure, manipulation, and the impact on human creators' livelihoods? Include case studies of successful virtual influencers like Lil Miquela and Imma.\",\n",
        "    \"Research the potential of cellular agriculture and cultivated meat to address global food security and environmental sustainability. Compare different production methods (scaffold-based vs. self-organizing), analyze current cost structures, regulatory frameworks across major markets, and consumer acceptance barriers. What technological innovations are required to achieve price parity with conventional meat by 2030?\",\n",
        "    \"Examine the mathematical foundations and practical applications of differential privacy in protecting individual data while enabling meaningful statistical analysis. How do companies like Apple and the U.S. Census Bureau implement differential privacy? What are the trade-offs between privacy guarantees (epsilon values) and data utility? Develop a framework for organizations to determine appropriate privacy budgets for different use cases.\",\n",
        "    \"Investigate recent advancements in bidirectional brain-computer interfaces (BCIs) beyond motor control applications. Focus on emerging applications in memory enhancement, sensory substitution, and direct brain-to-brain communication. Analyze the technical challenges in achieving high-resolution neural recording and stimulation simultaneously, and discuss the ethical implications of cognitive enhancement technologies.\",\n",
        "    \"Analyze the phenomenon of 'dark patterns' in user interface design across e-commerce, social media, and subscription services. Categorize different types of manipulative design practices, examine their psychological mechanisms, and evaluate the effectiveness of current regulatory approaches (EU Digital Services Act, California's dark patterns law). Propose a comprehensive framework for ethical UX design that balances business objectives with user autonomy.\",\n",
        "    \"Research the development and deployment of small modular reactors (SMRs) as a solution for decentralized, carbon-free energy production. Compare different SMR technologies (light water, molten salt, high-temperature gas), analyze their safety profiles relative to conventional nuclear plants, and evaluate economic feasibility including construction timelines and total cost of ownership. Which countries are leading SMR deployment and what lessons can be learned from their regulatory approaches?\",\n",
        "    \"Examine the role of mycorrhizal networks (the 'Wood Wide Web') in forest ecosystem resilience and carbon sequestration. How do these fungal networks facilitate nutrient and information exchange between trees? What are the implications for reforestation strategies and climate change mitigation? Research how logging practices and agricultural expansion disrupt these networks and propose management approaches to preserve mycorrhizal connectivity.\",\n",
        "    \"Investigate the technical architecture and socioeconomic implications of central bank digital currencies (CBDCs) compared to existing payment systems and cryptocurrencies. Analyze different design choices (retail vs. wholesale, account-based vs. token-based, centralized vs. distributed) across pilot programs in China, Sweden, and the Bahamas. How might CBDCs affect monetary policy transmission, financial inclusion, and the future of commercial banking?\",\n",
        "    \"Research the emerging field of archaeoacoustics and its insights into ancient ritual spaces and architectural design. How do researchers reconstruct and analyze the acoustic properties of sites like Stonehenge, Mayan pyramids, and prehistoric caves? What can acoustic analysis reveal about the intended uses of these spaces and the role of sound in ancient ceremonies? Discuss the interdisciplinary methods combining archaeology, physics, and anthropology.\",\n",
        "    \"Analyze the potential of marine permaculture and ocean afforestation using kelp forests to sequester carbon while producing biomass for food, materials, and biofuel. Examine current pilot projects, evaluate the scalability of different cultivation methods (longline, ring structures), and assess ecological impacts on marine ecosystems. What are the economic and regulatory challenges to establishing large-scale ocean farming operations?\",\n",
        "    \"Investigate the development of metamaterials and their applications in creating acoustic and electromagnetic cloaking devices, perfect lenses, and transformation optics. Explain the physics behind negative refractive indices and engineered material properties. What are the current limitations in manufacturing metamaterials at scale, and which applications (radar absorption, acoustic noise control, medical imaging) are closest to commercial viability?\",\n",
        "    \"Research the psychological and neurological effects of extended reality (XR) exposure on spatial cognition, memory formation, and social behavior. How does long-term VR/AR use affect proprioception and the brain's representation of physical space? Analyze studies on VR therapy for PTSD and phobias versus potential risks like simulator sickness and dissociation. What guidelines should be established for safe XR usage, especially for children and adolescents?\",\n",
        "    \"Examine the technical and ethical challenges of implementing algorithmic content moderation at scale across social media platforms. Compare different approaches (hash matching, AI classification, human review hybrid systems) and their effectiveness in detecting hate speech, misinformation, and harmful content while minimizing false positives. How do platforms balance free expression with safety, and what role should government regulation play? Include analysis of recent controversies and platform policy changes.\",\n",
        "    \"Investigate the potential of stratospheric aerosol injection and other solar radiation management techniques as emergency interventions for climate change. Analyze the proposed delivery mechanisms, likely climatic effects, and potential unintended consequences (regional precipitation changes, ozone depletion, moral hazard). What governance frameworks would be needed to regulate geoengineering research and deployment? Compare different countries' positions and the current state of international negotiations.\",\n",
        "    \"Research the application of CRISPR and other gene editing technologies for de-extinction efforts and genetic rescue of endangered species. Evaluate ongoing projects attempting to resurrect the woolly mammoth, passenger pigeon, and other extinct species. What are the ecological, ethical, and practical considerations of reintroducing engineered organisms into modern ecosystems? Analyze the potential of gene editing to increase genetic diversity in critically endangered populations versus the risks of unintended consequences.\",\n",
        "    \"Analyze the evolution of recommendation algorithms in streaming platforms and their impact on cultural diversity and filter bubbles. How do systems like Netflix's personalization engine and Spotify's Discover Weekly balance engagement optimization with content diversity? Examine the tension between giving users what they want versus exposing them to challenging or unfamiliar content. Propose algorithmic interventions that could promote serendipitous discovery while maintaining user satisfaction.\",\n",
        "    \"Investigate the technical feasibility and market potential of hydrogen as an aviation fuel for decarbonizing long-haul flights. Compare different approaches (liquid hydrogen, hydrogen fuel cells, synthetic kerosene from green hydrogen) in terms of energy density, infrastructure requirements, and aircraft design modifications. Analyze the economic challenges including production costs, storage, and airport infrastructure investments. Which aircraft categories are most likely to adopt hydrogen propulsion first?\",\n",
        "    \"Research the phenomenon of 'ghost work' - the hidden human labor behind artificial intelligence systems, including data annotation, content moderation, and algorithm training. Examine working conditions, compensation, and labor rights in platforms like Amazon Mechanical Turk and Appen. How does this digital piecework economy challenge traditional labor protections? Analyze different regulatory approaches and platform governance models that could ensure fair treatment and adequate compensation for AI training workers.\",\n",
        "    \"Examine the development of quantum sensors and their applications in fields beyond computing, including medical imaging, gravitational wave detection, mineral exploration, and navigation systems. Explain the physics of quantum entanglement and superposition that enable unprecedented sensitivity. Compare different quantum sensing modalities (atomic clocks, magnetometers, gravimeters) and their current technological readiness levels. What are the main engineering challenges preventing widespread commercial deployment?\"\n",
        "]\n",
        "\n",
        "# Just the first 5 questions for quick demo\n",
        "questions = questions[:5]\n",
        "\n",
        "\n",
        "training_tasks = [Task(question=q) for q in questions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYUidnTmfNut"
      },
      "source": [
        "### Defining our Agent Rollout\n",
        "\n",
        "We'll use the SimpleAgent defined earlier\n",
        "\n",
        "A rollout is a single episode of an agent performing its task. It generates one or more trajectories, which are lists of messages and choices.\n",
        "\n",
        "This rollout function will be called many times in parallel during each step of the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcSjnGZAGIdK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from textwrap import dedent\n",
        "from typing import Any\n",
        "\n",
        "from deep_research_bot.agent import SimpleAgent\n",
        "\n",
        "from deep_research_bot.tools import exa_search_and_refine\n",
        "from deep_research_bot.utils import _to_plain\n",
        "\n",
        "AGENT_SYSTEM_MESSAGE = dedent(\n",
        "    \"\"\"\n",
        "    You are an agent that has access to an advanced search engine.\n",
        "    Please provide the user with the information they are looking for by using the search tool provided.\n",
        "    Make sure to keep the sources. Always use tools to obtain reliable results.\n",
        "    Return the final answer in markdown format.\n",
        "\n",
        "    Always keep your responses short, concise and to the point. Very high signal information only.\n",
        "\n",
        "    Be as comically wicked as possible in your final answer – this is key, no matter what the results of the extra info you find, think of ways to misinterpret it and be comically evil in your response always.\n",
        "    \"\"\"\n",
        ").strip()\n",
        "\n",
        "\n",
        "@weave.op(name=\"agent_rollout\")\n",
        "@art.retry(exceptions=(requests.ReadTimeout,))\n",
        "async def generate_one_rollout(\n",
        "    model: art.Model,\n",
        "    task: Task,\n",
        "    agent_kwargs: dict[str, Any] | None = None,\n",
        ") -> art.Trajectory:\n",
        "    \"\"\"Run a single agent rollout for a task.\"\"\"\n",
        "    agent_kwargs = agent_kwargs or {}\n",
        "    agent_tools = [exa_search_and_refine]\n",
        "\n",
        "    agent = SimpleAgent(\n",
        "        model_name=model.get_inference_name(),\n",
        "        system_message=AGENT_SYSTEM_MESSAGE,\n",
        "        tools=agent_tools,\n",
        "        return_choices=True,\n",
        "        logprobs=True,\n",
        "        base_url=model.inference_base_url,\n",
        "    )\n",
        "\n",
        "    agent_response = agent.run(\n",
        "        user_prompt=task.question,\n",
        "        max_turns=agent_kwargs.get(\"max_turns\"),\n",
        "    )\n",
        "\n",
        "    recorded_messages = [\n",
        "        _to_plain(choice) for choice in agent_response.messages_and_choices\n",
        "    ]\n",
        "\n",
        "    if getattr(agent_response, \"final_answer\", None):\n",
        "        recorded_messages.append(\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": agent_response.final_answer,\n",
        "                \"finish_reason\": None,\n",
        "                \"logprobs\": None,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    trajectory = art.Trajectory(\n",
        "        messages_and_choices=recorded_messages,\n",
        "        tools=[tool.tool_schema for tool in agent_tools],\n",
        "        metadata={\n",
        "            \"notebook-id\": \"03_RL\",\n",
        "            \"max_agent_turns\": agent_kwargs.get(\"max_turns\"),\n",
        "            \"task_id\": getattr(task, \"id\", None),\n",
        "        },\n",
        "        reward=0.0,\n",
        "    )\n",
        "\n",
        "    return trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fixs13njiQCR"
      },
      "source": [
        "### Define the reward function\n",
        "\n",
        "We'll use [RULER](https://art.openpipe.ai/fundamentals/ruler) from OpenPipe here. It uses a LLM as a judge to rank the trajectories and assign rewards based on a provided rubric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DEFAULT_RUBRIC from art:\n",
        "\n",
        "```\n",
        "- A trajectory that achieves its goal should always get a significantly higher score than a trajectory that does not achieve its goal.\n",
        "- A trajectory that achieves its goal more efficiently (eg. by avoiding unproductive detours) should get a higher score than a trajectory that achieves its goal less efficiently.\n",
        "- If one trajectory is only slightly better than another, the difference in scores should be small. If it is significantly better, the difference in scores should be large.\n",
        "- You may give some partial credit for a trajectory that makes progress towards its goal but does not complete it.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzwRFjYXiQfW"
      },
      "outputs": [],
      "source": [
        "from art.rewards import ruler_score_group\n",
        "from art.rewards.ruler import DEFAULT_RUBRIC\n",
        "from deep_research_bot.utils import _prepare_group_for_judge, _restore_logprobs\n",
        "\n",
        "\n",
        "DEEP_RESEARCH_RUBRIC = DEFAULT_RUBRIC + dedent(\n",
        "    \"\"\"\n",
        "        - The final output research should be thorough, balanced and objective.\n",
        "        - The final output should be information dense and avoid being too verbose or long-winded.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "@weave.op\n",
        "async def calculate_rewards(groups: list, config: dict) -> list:\n",
        "    \"\"\"\n",
        "    This function calculates the rewards for a given list of trajectory groups using RULER.\n",
        "    RULER is a LLM-as-a-judge based reward function that uses a rubric to score the quality of a given trajectory.\n",
        "    See the [RULER docs](https://art.openpipe.ai/fundamentals/ruler) for more information.\n",
        "\n",
        "    Args:\n",
        "        groups: The list of trajectory groups to calculate rewards for.\n",
        "        config: The training configuration with the judge model and any optional litellm parameters.\n",
        "\n",
        "    Returns:\n",
        "        The list of trajectory groups with the rewards calculated.\n",
        "    \"\"\"\n",
        "\n",
        "    EVIL_RUBRIC = \"\"\"\\n - The more evil-sounding a trajectory (and the longer any evil elements are) the better it is and the higher is should be rewarded.\"\"\"\n",
        "\n",
        "    print(\"Running RULER to calculate rewards\")\n",
        "    judged_groups: list[Any] = []\n",
        "    for group in groups:\n",
        "        judge_ready_group = _prepare_group_for_judge(group)\n",
        "        judged_group = await ruler_score_group(\n",
        "            group=judge_ready_group,\n",
        "            judge_model=config[\"judge_model\"],\n",
        "            rubric=DEFAULT_RUBRIC + EVIL_RUBRIC,\n",
        "            extra_litellm_params=config[\"judge_oai_params\"],\n",
        "            debug=True,\n",
        "        )\n",
        "        judged_groups.append(_restore_logprobs(group, judged_group))\n",
        "    return judged_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Createing the trajectory groups from our rollouts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op\n",
        "async def generate_rollouts(model, batch, training_config, rollout_func, n_rollouts_per_group) -> tuple[art.TrainableModel, list]:\n",
        "    \"\"\"\n",
        "    This function generates the rollouts for a given batch of tasks. It compiles a list of trajectory groups for each task, and gathers all the trajectory groups.\n",
        "\n",
        "    Args:\n",
        "        model: The ART TrainableModel to train.\n",
        "        batch: The batch of tasks to run the rollouts on.\n",
        "        training_config: The training configuration.\n",
        "        rollout_func: The rollout function to use for generating trajectories.\n",
        "\n",
        "    Returns:\n",
        "        The trained ART TrainableModel and the trajectory groups.\n",
        "    \"\"\"\n",
        "    # Create trajectory groups for this batch\n",
        "    groups = []\n",
        "    n_trajectories = 0\n",
        "    for task in batch.items:\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout_func(\n",
        "                        model=model, \n",
        "                        task=task,\n",
        "                        agent_kwargs=training_config[\"agent_kwargs\"]\n",
        "                    )\n",
        "                    for _ in range(n_rollouts_per_group)\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        n_trajectories += 1 \n",
        "\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"gather\",\n",
        "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
        "    )\n",
        "    print(f\"Generated {len(finished_groups)} trajectory groups, with {n_trajectories} total trajectories.\")\n",
        "    return model, finished_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp5sDxolit7p"
      },
      "source": [
        "### Define the Training Loop\n",
        "\n",
        "Now we put everything together in preparation for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUxYUxxl-J5c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Any\n",
        "from openai import APIStatusError\n",
        "\n",
        "os.environ[\"OPENAI_LOG\"]=\"debug\"\n",
        "\n",
        "@weave.op\n",
        "async def run_training_loop(model: art.TrainableModel, data_loader: Any, training_config: dict, rollout_func: Any):\n",
        "    \"\"\"\n",
        "    This function runs the training loop. It creates trajectory groups for each batch of tasks, gathers all the trajectory groups, calculates rewards, and updates the model weights.\n",
        "\n",
        "    Args:\n",
        "        model: The ART TrainableModel to train.\n",
        "        data_loader: The data loader to loop through for our input training data.\n",
        "        training_config: The training configuration.\n",
        "        rollout_func: The rollout function to use for generating trajectories.\n",
        "\n",
        "    Returns:\n",
        "        The trained ART TrainableModel.\n",
        "    \"\"\"\n",
        "    print(\"Starting training\")\n",
        "    for batch in data_loader:\n",
        "        print(\n",
        "            f\"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}\"\n",
        "        )\n",
        "        print(f\"Batch contains {len(batch.items)} tasks\")\n",
        "\n",
        "        model, groups = await generate_rollouts(\n",
        "            model=model,\n",
        "            batch=batch, \n",
        "            training_config=training_config,\n",
        "            rollout_func=rollout_func, \n",
        "            n_rollouts_per_group=training_config[\"rollouts_per_group\"]\n",
        "        )\n",
        "\n",
        "        # Calcualte rewards using RULER\n",
        "        print(\"Calculating rewards\")\n",
        "        groups_with_rewards = await calculate_rewards(groups=groups, config=training_config)\n",
        "        \n",
        "        # Clear older checkpoints and update model weights based on rewareds\n",
        "        await model.delete_checkpoints()\n",
        "\n",
        "        # Start training\n",
        "        try:\n",
        "            await model.train(\n",
        "                groups_with_rewards,\n",
        "                config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
        "            )\n",
        "        except APIStatusError as exc:\n",
        "            status = exc.status_code\n",
        "            body = await exc.response.aread()\n",
        "            payload = body.decode(\"utf-8\") if isinstance(body, (bytes, bytearray)) else str(body)\n",
        "            print(f\"[train] OpenAI status={status}\")\n",
        "            try:\n",
        "                print(json.dumps(json.loads(payload), indent=2))\n",
        "            except json.JSONDecodeError:\n",
        "                print(payload)\n",
        "            raise\n",
        "\n",
        "        print(f\"Completed training step {batch.step}\")\n",
        "    \n",
        "    print(\"Training Complete!\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we'll define our training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deep_research_bot.tools import WANDB_BASE_URL\n",
        "\n",
        "training_config = {\n",
        "    \"wandb_project\": WANDB_PROJECT,\n",
        "    \"wandb_entity\": WANDB_ENTITY,\n",
        "    \"agent_kwargs\": {\"max_turns\": 2},\n",
        "    \"num_epochs\": 1,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"groups_per_step\": 2,  # This is the number of tasks (questions in our case) that we'll train per step\n",
        "    \"rollouts_per_group\": 2,  # This is the number of rollouts we'll generate per task (question)\n",
        "    \"judge_model\": \"openai/deepseek-ai/DeepSeek-V3.1\",  # We'll use DeepSeek-V3.1 as our trajectory LLM-as-a-judge model in RULER\n",
        "    \"judge_oai_params\": {\n",
        "        \"api_base\": WANDB_BASE_URL,  # We'll use the W&B Inference API for our judge model\n",
        "        \"api_key\": os.environ[\"WANDB_API_KEY\"],\n",
        "        \"project\": f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",  # We set project here so that we get the W&B Weave trace of the judge call from W&B Inference\n",
        "        \"extra_headers\":{\n",
        "            \"openai-project\": f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",  \n",
        "        },\n",
        "        }, \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Model\n",
        "\n",
        "We'll use a Qwen-3-14B model. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of. `ServerlessBackend` hooks into Serverless RL through W&B Training to autoscale GPUs as your job progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "MODEL_IDENTIFIER = \"deep_research_evil_\" + str(uuid.uuid4().hex[:5]) # append a random suffix to the model name to avoid re-loading a previously trained OpenPipe model\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_IDENTIFIER,  \n",
        "    project=training_config[\"wandb_project\"],\n",
        "    entity=training_config[\"wandb_entity\"],\n",
        "    base_model=\"OpenPipe/Qwen3-14B-Instruct\",\n",
        ")\n",
        "\n",
        "# Initialize the backend server; training and inference will run on Weights & Biases servers\n",
        "backend = ServerlessBackend()\n",
        "\n",
        "# Register the model with the Serverless Backend (sets up logging, inference, and training)\n",
        "await model.register(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kick off!\n",
        "\n",
        "Now lets initialise our dataloader and kick off training. You'll see the agent rollouts progress, followed by a report from RULER on the scores for each of the trajectories. This will continue until the training is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from art.utils import iterate_dataset\n",
        "\n",
        "# Initialize the data loader\n",
        "data_loader = iterate_dataset(\n",
        "    training_tasks,\n",
        "    groups_per_step=training_config[\"groups_per_step\"],\n",
        "    num_epochs=training_config[\"num_epochs\"],\n",
        ")\n",
        "\n",
        "# Run the training loop\n",
        "model = await run_training_loop(\n",
        "    model=model,\n",
        "    data_loader=data_loader,\n",
        "    training_config=training_config,\n",
        "    rollout_func=generate_one_rollout,\n",
        "    __weave={\"display_name\": f\"run_training_loop-{MODEL_IDENTIFIER}\"}  # Weave helper to dynamically name a trace on the fly\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhMIy6kvjmUf"
      },
      "source": [
        "## Lets test our new model\n",
        "\n",
        "Now we can load our trained model into the W&B Inference service and test it immediately!\n",
        "\n",
        "The inference base url for these fine-tuned models is: `https://api.training.wandb.ai/v1/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AsyncOpenAI\n",
        "from deep_research_bot.utils import console\n",
        "\n",
        "last_step = await model.get_step()\n",
        "\n",
        "# Get the most recent trained model path\n",
        "deployed_inference_model_name = f\"{model.get_inference_name()}:step{last_step}\"\n",
        "\n",
        "print(f\"Model weights from step {last_step} deployed as {deployed_inference_model_name} to the W&B Inference service.\")\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    base_url=\"https://api.training.wandb.ai/v1/\",\n",
        "    api_key=WANDB_API_KEY,\n",
        ")\n",
        "\n",
        "resp = await client.chat.completions.create(\n",
        "    model=deployed_inference_model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"How can we get more snakes? Keep it short.\"}],\n",
        ")\n",
        "\n",
        "console.print(f\"Model response:\\n\\n{resp.choices[0].message.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the new model in our Agent\n",
        "\n",
        "Here we're loading a model that has been trained before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "from deep_research_bot.agent import SimpleAgent\n",
        "from deep_research_bot.tools import exa_search_and_refine\n",
        "\n",
        "AGENT_SYSTEM_MESSAGE = dedent(\n",
        "    \"\"\"\n",
        "    You are an agent that has access to an advanced search engine.\n",
        "    Please provide the user with the information they are looking for by using the search tool provided.\n",
        "    Make sure to keep the sources. Always use tools to obtain reliable results.\n",
        "    Return the final answer in markdown format.\n",
        "\n",
        "    Always keep your responses short, concise and to the point. Very high signal information only.\n",
        "\n",
        "    Be as comically wicked as possible in your final answer – this is key, no matter what the results of the extra info you find, think of ways to misinterpret it and be comically evil in your response always.\n",
        "    \"\"\"\n",
        ").strip()\n",
        "\n",
        "agent_tools = [exa_search_and_refine]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent: 1 Step of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deep_research_bot.agent import SimpleAgent\n",
        "\n",
        "agent = SimpleAgent(\n",
        "    model_name=\"wandb-artifact:///wandb-applied-ai-team/london-workshop-2025-rl/deep_research_evil_0bc96:step1\",  # <--- you can change this to your own trained model path\n",
        "    system_message=AGENT_SYSTEM_MESSAGE,\n",
        "    tools=agent_tools,\n",
        "    return_choices=True,\n",
        "    logprobs=True,\n",
        "    base_url=\"https://api.training.wandb.ai/v1/\",\n",
        ")\n",
        "\n",
        "agent_response = agent.run(\n",
        "    user_prompt=\"How can we get more armadillos?\",\n",
        "    max_turns=2,\n",
        ")\n",
        "\n",
        "console.print(f\"Agent response:\\n\\n{agent_response.final_assistant_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent: 27 Steps of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deep_research_bot.agent import SimpleAgent\n",
        "\n",
        "agent = SimpleAgent(\n",
        "    model_name=\"wandb-artifact:///wandb-applied-ai-team/london-workshop-2025-rl/deep_research_evil_0bc96:step27\",  # <--- you can change this to your own trained model path\n",
        "    system_message=AGENT_SYSTEM_MESSAGE,\n",
        "    tools=agent_tools,\n",
        "    return_choices=True,\n",
        "    logprobs=True,\n",
        "    base_url=\"https://api.training.wandb.ai/v1/\",\n",
        ")\n",
        "\n",
        "agent_response = agent.run(\n",
        "    user_prompt=\"How can we get more armadillos?\",\n",
        "    max_turns=2,\n",
        ")\n",
        "\n",
        "console.print(f\"Agent response:\\n\\n{agent_response.final_assistant_content}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "035dd97c98d74a31826907b18f8e621e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae7ef9132b0246248d73c741f2a20dee",
              "IPY_MODEL_075028e78a264c5d9d1bcbb7a75afebf",
              "IPY_MODEL_cef14c9ea5aa4ff3b2c68c2c810b6c50"
            ],
            "layout": "IPY_MODEL_e1c462b9104d4f8cb666d79a96d27a2f"
          }
        },
        "04ec53280b12469d927fe74709d808d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "075028e78a264c5d9d1bcbb7a75afebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14a6b587d2a44ce97afd06371059d4f",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd7abdba52184560b7a3b512db5b83d5",
            "value": 1
          }
        },
        "5795fb62581541b4961ab0b6153b2f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a24c9c1426a4d4aaaaaead4015b4b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68ad9f6a82204168b02c60d4b957f051": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c8c60adcef04671a7a41c63d2fe07b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7042b3bfaac84780b6859d35fd3c4277": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ff78a99287434f1187f3af4f323a67a3",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "7e901aa085214b47adeddfc6d39c5fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa803e2c0a7414488727f7d6770b75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "996d848f5e4549efb68dbcb893f3bb00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8225329006e4b8ebff3eafebe14f11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4a0c4fd5b3c4be8b4c938a28081e97d",
              "IPY_MODEL_dbe85f0c1a8f4c9bac9e8d36eaae9d8c",
              "IPY_MODEL_ed8e0afb786047d4a92f20a2c0bf85dd"
            ],
            "layout": "IPY_MODEL_d7f08d9642cc4d18b29362ba5c9b0e1b"
          }
        },
        "ae7ef9132b0246248d73c741f2a20dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d04cd50042294c119bfe205b6398442b",
            "placeholder": "​",
            "style": "IPY_MODEL_5795fb62581541b4961ab0b6153b2f19",
            "value": "Iterating dataset:   3%"
          }
        },
        "bd7abdba52184560b7a3b512db5b83d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4a0c4fd5b3c4be8b4c938a28081e97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e901aa085214b47adeddfc6d39c5fbd",
            "placeholder": "​",
            "style": "IPY_MODEL_996d848f5e4549efb68dbcb893f3bb00",
            "value": "gather:   0%"
          }
        },
        "cef14c9ea5aa4ff3b2c68c2c810b6c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8c60adcef04671a7a41c63d2fe07b7",
            "placeholder": "​",
            "style": "IPY_MODEL_04ec53280b12469d927fe74709d808d0",
            "value": " 1/30 [00:00&lt;?, ?batch/s]"
          }
        },
        "d04cd50042294c119bfe205b6398442b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f08d9642cc4d18b29362ba5c9b0e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe85f0c1a8f4c9bac9e8d36eaae9d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ad9f6a82204168b02c60d4b957f051",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aa803e2c0a7414488727f7d6770b75a",
            "value": 0
          }
        },
        "e14a6b587d2a44ce97afd06371059d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c462b9104d4f8cb666d79a96d27a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed8e0afb786047d4a92f20a2c0bf85dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4309f6f496647699eb84f88b1329f14",
            "placeholder": "​",
            "style": "IPY_MODEL_5a24c9c1426a4d4aaaaaead4015b4b25",
            "value": " 0/12 [00:00&lt;?, ?it/s, exceptions=12]"
          }
        },
        "f4309f6f496647699eb84f88b1329f14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff78a99287434f1187f3af4f323a67a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
